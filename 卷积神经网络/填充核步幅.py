# 此时的填充和步幅应该都是超参数
# 此时的填充的值p_h为1,代表上填充一个，下面填充一个
# 此时加上填充和步幅的超参数时输出的求解：y_h=floor((x_h-k_h+2*p_h)/s_h)+1
# 此时当p_h的取值为2*p_h=k_h-1时，图片的输出尺寸为原来的 x/s
# 一般来说减半是很少进行，较为常见的都是保持不变，步幅一般选择为1
# 对于核的大小的选择一般是保证为 3*3或者 5*5 ，padding的选择为(k_size-1)/2

# 在单输入，单输出的情况下：
# 选用的核取较小的原因是：原输入（6*6）通过3*3的卷积核使得输出层1的形状为4*4，再通过3*3的卷积核使输出变为了输出层2的2*2（步长都为1）
# 输出2中的一个元素来自于输出层1的9个元素，输出层2对应的输出层1的9个元素的其中一个来自于输入的9个元素，由于步长为1，可以得到输出2的一个元素包含了原输入的25个元素
# 因此我们会选择深度较深，核的大小较小的网络
# 同时在选择卷积核的大小时应当优先考虑使用size较小的核（在输出保证原像素的情况），2个3*3可以等价为5*5的kernel
# 原因是：2个3*3可以等价为5*5的kernel，计算成本为k_h*k_w*y_h*y_w，此时y_h*y_w保证不变时，利用3*3每次计算成本为3*3*n*y_h*y_w,利用5*5的计算成本为5*5*y_h*y_w，3*3计算成本较小，n不会大于2的

# 整一个深度学习，机器学习做的就是将一张图片压缩到一个标签（信息筛选），一定会出现信息的丢失
# 一个特定的卷积层相当于可以去匹配一种特定的纹理（特定的特征）

# 通常来说若果通过卷积层后宽高不变，那么通道数不变，如果宽高减少一倍，则通道数会增加一倍
from torch import nn
import torch
def comp_conv(X,conv2d):
    X=torch.reshape(X,((1,1)+X.shape))
    Y=conv2d(X)
    Y=torch.reshape(Y,Y.shape[2:])
    print(Y.shape)

X=torch.randn((8,8))
conv2d=nn.Conv2d(1,1,kernel_size=(3,3),padding=1,stride=2)
#conv2d=nn.Conv2d(1,1,kernel_size=(3,3),padding='same') # 如果padding选择same时，必须为stride=1
comp_conv(X,conv2d)  #输出的形状应当为（8-3+2）//2+1=4